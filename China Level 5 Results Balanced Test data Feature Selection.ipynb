{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Particulate Matter (PM2.5) Concentrations in the Air of China"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries necessary for this project\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# Dara preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#from sklearn.model_selection import cross_validate\n",
    "# Import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import svm\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "from math import sqrt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records for all Chines cities:  117200\n",
      "*********************\n",
      "Number of records for all Chines cities: Training :  93761\n",
      "*********************\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 93761 entries, 0 to 93760\n",
      "Data columns (total 28 columns):\n",
      "DEWP             93761 non-null float64\n",
      "HUMI             93761 non-null float64\n",
      "PM_US Post       93761 non-null float64\n",
      "PRES             93761 non-null float64\n",
      "TEMP             93761 non-null float64\n",
      "Weekdays         93761 non-null int64\n",
      "Weekends         93761 non-null int64\n",
      "cbwd_NE          93761 non-null int64\n",
      "cbwd_NW          93761 non-null int64\n",
      "cbwd_SE          93761 non-null int64\n",
      "cbwd_SW          93761 non-null int64\n",
      "cbwd_cv          93761 non-null int64\n",
      "day              93761 non-null int64\n",
      "day_cos          93761 non-null float64\n",
      "day_sin          93761 non-null float64\n",
      "hour             93761 non-null int64\n",
      "hour_cos         93761 non-null float64\n",
      "hour_sin         93761 non-null float64\n",
      "month            93761 non-null int64\n",
      "month_cos        93761 non-null float64\n",
      "month_sin        93761 non-null float64\n",
      "new_wind         93761 non-null float64\n",
      "precipitation    93761 non-null float64\n",
      "season_1         93761 non-null int64\n",
      "season_2         93761 non-null int64\n",
      "season_3         93761 non-null int64\n",
      "season_4         93761 non-null int64\n",
      "year             93761 non-null int64\n",
      "dtypes: float64(13), int64(15)\n",
      "memory usage: 20.0 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEWP</th>\n",
       "      <th>HUMI</th>\n",
       "      <th>PM_US Post</th>\n",
       "      <th>PRES</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>Weekdays</th>\n",
       "      <th>Weekends</th>\n",
       "      <th>cbwd_NE</th>\n",
       "      <th>cbwd_NW</th>\n",
       "      <th>cbwd_SE</th>\n",
       "      <th>...</th>\n",
       "      <th>month</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>new_wind</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>season_1</th>\n",
       "      <th>season_2</th>\n",
       "      <th>season_3</th>\n",
       "      <th>season_4</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>93761.000000</td>\n",
       "      <td>93761.000000</td>\n",
       "      <td>93761.000000</td>\n",
       "      <td>93761.000000</td>\n",
       "      <td>93761.000000</td>\n",
       "      <td>93761.000000</td>\n",
       "      <td>93761.000000</td>\n",
       "      <td>93761.000000</td>\n",
       "      <td>93761.000000</td>\n",
       "      <td>93761.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>93761.000000</td>\n",
       "      <td>9.376100e+04</td>\n",
       "      <td>93761.000000</td>\n",
       "      <td>93761.000000</td>\n",
       "      <td>93761.000000</td>\n",
       "      <td>93761.000000</td>\n",
       "      <td>93761.000000</td>\n",
       "      <td>93761.000000</td>\n",
       "      <td>93761.000000</td>\n",
       "      <td>93761.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.973951</td>\n",
       "      <td>66.601146</td>\n",
       "      <td>71.017897</td>\n",
       "      <td>1013.578165</td>\n",
       "      <td>16.175369</td>\n",
       "      <td>0.716033</td>\n",
       "      <td>0.283967</td>\n",
       "      <td>0.222001</td>\n",
       "      <td>0.246979</td>\n",
       "      <td>0.221179</td>\n",
       "      <td>...</td>\n",
       "      <td>6.533740</td>\n",
       "      <td>7.335476e-03</td>\n",
       "      <td>-0.008719</td>\n",
       "      <td>2.757414</td>\n",
       "      <td>0.124664</td>\n",
       "      <td>0.248835</td>\n",
       "      <td>0.242094</td>\n",
       "      <td>0.254530</td>\n",
       "      <td>0.254541</td>\n",
       "      <td>2014.009236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.326445</td>\n",
       "      <td>22.355580</td>\n",
       "      <td>65.313242</td>\n",
       "      <td>9.907536</td>\n",
       "      <td>10.745159</td>\n",
       "      <td>0.450923</td>\n",
       "      <td>0.450923</td>\n",
       "      <td>0.415594</td>\n",
       "      <td>0.431257</td>\n",
       "      <td>0.415043</td>\n",
       "      <td>...</td>\n",
       "      <td>3.476925</td>\n",
       "      <td>7.053905e-01</td>\n",
       "      <td>0.708735</td>\n",
       "      <td>1.944073</td>\n",
       "      <td>1.120329</td>\n",
       "      <td>0.432340</td>\n",
       "      <td>0.428353</td>\n",
       "      <td>0.435599</td>\n",
       "      <td>0.435605</td>\n",
       "      <td>0.809671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-40.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>975.000000</td>\n",
       "      <td>-25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2013.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>51.560000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>1006.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2013.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>70.360000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2014.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>1021.000000</td>\n",
       "      <td>24.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>8.660254e-01</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>932.000000</td>\n",
       "      <td>1046.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.120000</td>\n",
       "      <td>48.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               DEWP          HUMI    PM_US Post          PRES          TEMP  \\\n",
       "count  93761.000000  93761.000000  93761.000000  93761.000000  93761.000000   \n",
       "mean       8.973951     66.601146     71.017897   1013.578165     16.175369   \n",
       "std       12.326445     22.355580     65.313242      9.907536     10.745159   \n",
       "min      -40.000000      2.000000      1.000000    975.000000    -25.000000   \n",
       "25%        1.000000     51.560000     29.000000   1006.000000      9.000000   \n",
       "50%       12.000000     70.360000     52.000000   1013.000000     18.000000   \n",
       "75%       19.000000     86.000000     90.000000   1021.000000     24.400000   \n",
       "max       28.000000    100.000000    932.000000   1046.000000     42.000000   \n",
       "\n",
       "           Weekdays      Weekends       cbwd_NE       cbwd_NW       cbwd_SE  \\\n",
       "count  93761.000000  93761.000000  93761.000000  93761.000000  93761.000000   \n",
       "mean       0.716033      0.283967      0.222001      0.246979      0.221179   \n",
       "std        0.450923      0.450923      0.415594      0.431257      0.415043   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        1.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        1.000000      1.000000      0.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       ...         month     month_cos     month_sin      new_wind  \\\n",
       "count  ...  93761.000000  9.376100e+04  93761.000000  93761.000000   \n",
       "mean   ...      6.533740  7.335476e-03     -0.008719      2.757414   \n",
       "std    ...      3.476925  7.053905e-01      0.708735      1.944073   \n",
       "min    ...      1.000000 -1.000000e+00     -1.000000      0.000000   \n",
       "25%    ...      3.000000 -5.000000e-01     -0.866025      1.100000   \n",
       "50%    ...      6.000000 -1.836970e-16      0.000000      2.000000   \n",
       "75%    ...     10.000000  8.660254e-01      0.500000      4.000000   \n",
       "max    ...     12.000000  1.000000e+00      1.000000     20.120000   \n",
       "\n",
       "       precipitation      season_1      season_2      season_3      season_4  \\\n",
       "count   93761.000000  93761.000000  93761.000000  93761.000000  93761.000000   \n",
       "mean        0.124664      0.248835      0.242094      0.254530      0.254541   \n",
       "std         1.120329      0.432340      0.428353      0.435599      0.435605   \n",
       "min         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%         0.000000      0.000000      0.000000      1.000000      1.000000   \n",
       "max        48.600000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "               year  \n",
       "count  93761.000000  \n",
       "mean    2014.009236  \n",
       "std        0.809671  \n",
       "min     2013.000000  \n",
       "25%     2013.000000  \n",
       "50%     2014.000000  \n",
       "75%     2015.000000  \n",
       "max     2015.000000  \n",
       "\n",
       "[8 rows x 28 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records for all Chines cities: Testing :  23439\n",
      "*********************\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23439 entries, 0 to 23438\n",
      "Data columns (total 28 columns):\n",
      "DEWP             23439 non-null float64\n",
      "HUMI             23439 non-null float64\n",
      "PM_US Post       23439 non-null float64\n",
      "PRES             23439 non-null float64\n",
      "TEMP             23439 non-null float64\n",
      "Weekdays         23439 non-null int64\n",
      "Weekends         23439 non-null int64\n",
      "cbwd_NE          23439 non-null int64\n",
      "cbwd_NW          23439 non-null int64\n",
      "cbwd_SE          23439 non-null int64\n",
      "cbwd_SW          23439 non-null int64\n",
      "cbwd_cv          23439 non-null int64\n",
      "day              23439 non-null int64\n",
      "day_cos          23439 non-null float64\n",
      "day_sin          23439 non-null float64\n",
      "hour             23439 non-null int64\n",
      "hour_cos         23439 non-null float64\n",
      "hour_sin         23439 non-null float64\n",
      "month            23439 non-null int64\n",
      "month_cos        23439 non-null float64\n",
      "month_sin        23439 non-null float64\n",
      "new_wind         23439 non-null float64\n",
      "precipitation    23439 non-null float64\n",
      "season_1         23439 non-null int64\n",
      "season_2         23439 non-null int64\n",
      "season_3         23439 non-null int64\n",
      "season_4         23439 non-null int64\n",
      "year             23439 non-null int64\n",
      "dtypes: float64(13), int64(15)\n",
      "memory usage: 5.0 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEWP</th>\n",
       "      <th>HUMI</th>\n",
       "      <th>PM_US Post</th>\n",
       "      <th>PRES</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>Weekdays</th>\n",
       "      <th>Weekends</th>\n",
       "      <th>cbwd_NE</th>\n",
       "      <th>cbwd_NW</th>\n",
       "      <th>cbwd_SE</th>\n",
       "      <th>...</th>\n",
       "      <th>month</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>new_wind</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>season_1</th>\n",
       "      <th>season_2</th>\n",
       "      <th>season_3</th>\n",
       "      <th>season_4</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>23439.000000</td>\n",
       "      <td>23439.000000</td>\n",
       "      <td>23439.000000</td>\n",
       "      <td>23439.000000</td>\n",
       "      <td>23439.000000</td>\n",
       "      <td>23439.00000</td>\n",
       "      <td>23439.00000</td>\n",
       "      <td>23439.000000</td>\n",
       "      <td>23439.000000</td>\n",
       "      <td>23439.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>23439.000000</td>\n",
       "      <td>2.343900e+04</td>\n",
       "      <td>23439.000000</td>\n",
       "      <td>23439.000000</td>\n",
       "      <td>23439.000000</td>\n",
       "      <td>23439.000000</td>\n",
       "      <td>23439.000000</td>\n",
       "      <td>23439.000000</td>\n",
       "      <td>23439.000000</td>\n",
       "      <td>23439.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.049435</td>\n",
       "      <td>66.617170</td>\n",
       "      <td>71.075643</td>\n",
       "      <td>1013.529634</td>\n",
       "      <td>16.257144</td>\n",
       "      <td>0.71108</td>\n",
       "      <td>0.28892</td>\n",
       "      <td>0.219335</td>\n",
       "      <td>0.246768</td>\n",
       "      <td>0.225692</td>\n",
       "      <td>...</td>\n",
       "      <td>6.553863</td>\n",
       "      <td>2.402283e-03</td>\n",
       "      <td>-0.011494</td>\n",
       "      <td>2.757413</td>\n",
       "      <td>0.123580</td>\n",
       "      <td>0.248944</td>\n",
       "      <td>0.244550</td>\n",
       "      <td>0.257989</td>\n",
       "      <td>0.248517</td>\n",
       "      <td>2014.019625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.285323</td>\n",
       "      <td>22.350348</td>\n",
       "      <td>66.554350</td>\n",
       "      <td>9.900487</td>\n",
       "      <td>10.692797</td>\n",
       "      <td>0.45327</td>\n",
       "      <td>0.45327</td>\n",
       "      <td>0.413805</td>\n",
       "      <td>0.431140</td>\n",
       "      <td>0.418046</td>\n",
       "      <td>...</td>\n",
       "      <td>3.463694</td>\n",
       "      <td>7.034655e-01</td>\n",
       "      <td>0.710662</td>\n",
       "      <td>1.940366</td>\n",
       "      <td>1.107444</td>\n",
       "      <td>0.432411</td>\n",
       "      <td>0.429829</td>\n",
       "      <td>0.437537</td>\n",
       "      <td>0.432163</td>\n",
       "      <td>0.810192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-38.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>980.300000</td>\n",
       "      <td>-25.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2013.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.600000</td>\n",
       "      <td>51.560000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>1006.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2013.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>70.470000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2014.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>85.230000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>1021.000000</td>\n",
       "      <td>24.600000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>756.000000</td>\n",
       "      <td>1046.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.200000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               DEWP          HUMI    PM_US Post          PRES          TEMP  \\\n",
       "count  23439.000000  23439.000000  23439.000000  23439.000000  23439.000000   \n",
       "mean       9.049435     66.617170     71.075643   1013.529634     16.257144   \n",
       "std       12.285323     22.350348     66.554350      9.900487     10.692797   \n",
       "min      -38.000000      2.000000      1.000000    980.300000    -25.000000   \n",
       "25%        1.600000     51.560000     29.000000   1006.000000      9.000000   \n",
       "50%       12.000000     70.470000     52.000000   1013.000000     18.000000   \n",
       "75%       19.000000     85.230000     90.000000   1021.000000     24.600000   \n",
       "max       28.000000    100.000000    756.000000   1046.000000     40.000000   \n",
       "\n",
       "          Weekdays     Weekends       cbwd_NE       cbwd_NW       cbwd_SE  \\\n",
       "count  23439.00000  23439.00000  23439.000000  23439.000000  23439.000000   \n",
       "mean       0.71108      0.28892      0.219335      0.246768      0.225692   \n",
       "std        0.45327      0.45327      0.413805      0.431140      0.418046   \n",
       "min        0.00000      0.00000      0.000000      0.000000      0.000000   \n",
       "25%        0.00000      0.00000      0.000000      0.000000      0.000000   \n",
       "50%        1.00000      0.00000      0.000000      0.000000      0.000000   \n",
       "75%        1.00000      1.00000      0.000000      0.000000      0.000000   \n",
       "max        1.00000      1.00000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       ...         month     month_cos     month_sin      new_wind  \\\n",
       "count  ...  23439.000000  2.343900e+04  23439.000000  23439.000000   \n",
       "mean   ...      6.553863  2.402283e-03     -0.011494      2.757413   \n",
       "std    ...      3.463694  7.034655e-01      0.710662      1.940366   \n",
       "min    ...      1.000000 -1.000000e+00     -1.000000      0.000000   \n",
       "25%    ...      4.000000 -5.000000e-01     -0.866025      1.100000   \n",
       "50%    ...      6.000000 -1.836970e-16      0.000000      2.000000   \n",
       "75%    ...     10.000000  5.000000e-01      0.500000      4.000000   \n",
       "max    ...     12.000000  1.000000e+00      1.000000     15.200000   \n",
       "\n",
       "       precipitation      season_1      season_2      season_3      season_4  \\\n",
       "count   23439.000000  23439.000000  23439.000000  23439.000000  23439.000000   \n",
       "mean        0.123580      0.248944      0.244550      0.257989      0.248517   \n",
       "std         1.107444      0.432411      0.429829      0.437537      0.432163   \n",
       "min         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%         0.000000      0.000000      0.000000      1.000000      0.000000   \n",
       "max        35.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "               year  \n",
       "count  23439.000000  \n",
       "mean    2014.019625  \n",
       "std        0.810192  \n",
       "min     2013.000000  \n",
       "25%     2013.000000  \n",
       "50%     2014.000000  \n",
       "75%     2015.000000  \n",
       "max     2015.000000  \n",
       "\n",
       "[8 rows x 28 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Guangzhou dataset\n",
    "Guangzhou = pd.read_csv('dataset_Guangzhou_clean.csv')\n",
    "Guangzhou = Guangzhou.drop('Unnamed: 0', axis = 1)\n",
    "\n",
    "# Load Beijing dataset\n",
    "Beijing = pd.read_csv('dataset_Beijing_clean.csv')\n",
    "Beijing = Beijing.drop('Unnamed: 0', axis = 1)\n",
    "\n",
    "\n",
    "# Load Chengdu dataset\n",
    "Chengdu = pd.read_csv('dataset_Chengdu_clean.csv')\n",
    "Chengdu = Chengdu.drop('Unnamed: 0', axis = 1)\n",
    "\n",
    "# Load Shanghai dataset\n",
    "Shanghai = pd.read_csv('dataset_Shanghai_clean.csv')\n",
    "Shanghai = Shanghai.drop('Unnamed: 0', axis = 1)\n",
    "\n",
    "# Load Shenyang dataset\n",
    "Shenyang = pd.read_csv('dataset_Shenyang_clean.csv')\n",
    "Shenyang = Shenyang.drop('Unnamed: 0', axis = 1)\n",
    "\n",
    "dataset = Beijing.append(Chengdu, ignore_index=True, sort=True)\n",
    "dataset = dataset.append(Shanghai, ignore_index=True, sort=True)\n",
    "dataset = dataset.append(Shenyang, ignore_index=True, sort=True)\n",
    "dataset = dataset.append(Guangzhou, ignore_index=True, sort=True)\n",
    "\n",
    "Beijing_testing_rows = Beijing.sample(frac=0.2)\n",
    "Beijing = Beijing.drop(Beijing.index[Beijing_testing_rows.index])\n",
    "\n",
    "Guangzhou_testing_rows = Guangzhou.sample(frac=0.2)\n",
    "Guangzhou = Guangzhou.drop(Guangzhou.index[Guangzhou_testing_rows.index])\n",
    "\n",
    "\n",
    "Chengdu_testing_rows = Chengdu.sample(frac=0.2)\n",
    "Chengdu = Chengdu.drop(Chengdu.index[Chengdu_testing_rows.index])\n",
    "\n",
    "\n",
    "Shanghai_testing_rows = Shanghai.sample(frac=0.2)\n",
    "Shanghai = Shanghai.drop(Shanghai.index[Shanghai_testing_rows.index])\n",
    "\n",
    "\n",
    "Shenyang_testing_rows = Shenyang.sample(frac=0.2)\n",
    "Shenyang = Shenyang.drop(Shenyang.index[Shenyang_testing_rows.index])\n",
    "\n",
    "\n",
    "train_dataset = Beijing.append(Chengdu, ignore_index=True, sort=True)\n",
    "train_dataset = train_dataset.append(Shanghai, ignore_index=True, sort=True)\n",
    "train_dataset = train_dataset.append(Shenyang, ignore_index=True, sort=True)\n",
    "train_dataset = train_dataset.append(Guangzhou, ignore_index=True, sort=True)\n",
    "\n",
    "test_dataset = Beijing_testing_rows.append(Chengdu_testing_rows, ignore_index=True, sort=True)\n",
    "test_dataset = test_dataset.append(Shanghai_testing_rows, ignore_index=True, sort=True)\n",
    "test_dataset = test_dataset.append(Shenyang_testing_rows, ignore_index=True, sort=True)\n",
    "test_dataset = test_dataset.append(Guangzhou_testing_rows, ignore_index=True, sort=True)\n",
    "\n",
    "\n",
    "# Total number of records\n",
    "n_records = len(dataset[\"PM_US Post\"])\n",
    "print(\"Number of records for all Chines cities: \", n_records)\n",
    "print(\"*********************\")\n",
    "\n",
    "# Total number of records\n",
    "n_records = len(train_dataset[\"PM_US Post\"])\n",
    "print(\"Number of records for all Chines cities: Training : \", n_records)\n",
    "print(\"*********************\")\n",
    "display(train_dataset.info())\n",
    "display(train_dataset.describe())\n",
    "\n",
    "# Total number of records\n",
    "n_records = len(test_dataset[\"PM_US Post\"])\n",
    "print(\"Number of records for all Chines cities: Testing : \", n_records)\n",
    "print(\"*********************\")\n",
    "display(test_dataset.info())\n",
    "display(test_dataset.describe())\n",
    "#print(len(Beijing))\n",
    "#print(len(Beijing_testing_fraction_of_rows))\n",
    "#print((Beijing_testing_rows['year'].value_counts()))\n",
    "#print(Beijing.head(n = 5))\n",
    "#print(Beijing_testing_rows.head(n = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('dataset_all_cities_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine learning algorithms decleration\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "\n",
    "LR = LinearRegression()\n",
    "RF = RandomForestRegressor(n_estimators = 96)\n",
    "ANN = MLPRegressor(hidden_layer_sizes= (128, 256))\n",
    "SVR = SVR(kernel='rbf')\n",
    "GNB = GaussianNB()\n",
    "KNN = KNeighborsRegressor(n_neighbors=5)\n",
    "#SVR_tuned = SVR(kernel='rbf', C = 707, epsilon = 4)\n",
    "\n",
    "MLs = {'LR' : LR, 'RF': RF, 'ANN' : ANN, 'SVR' : SVR}\n",
    "#MLs = {'LR' : LR}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def apply_L5(cityTrainName, Train_dataset, cityTestName, Test_dataset, MLname, estimator, f_out, Un_needed_columns):\n",
    "    # Construct the pipeline with a standard scaler and a small neural network\n",
    "    estimators = []\n",
    "    estimators.append(('standardize', StandardScaler()))\n",
    "    estimators.append((MLname, estimator))\n",
    "    model = Pipeline(estimators)\n",
    "\n",
    "    \n",
    "    # Split the data into features and target label\n",
    "    #Un_needed_columns = ['PM_US Post']\n",
    "    \n",
    "    # Split the data into features and target label\n",
    "    X_train = Train_dataset.drop(Un_needed_columns, axis = 1)\n",
    "    X_test = Test_dataset.drop(Un_needed_columns, axis = 1)\n",
    "    \n",
    "    y_train = Train_dataset['PM_US Post']\n",
    "    y_test = Test_dataset['PM_US Post']\n",
    "    \n",
    "    # Saving feature names for later use\n",
    "    features_list = list(X_train.columns)\n",
    "\n",
    "    print(estimators)\n",
    "\n",
    "    print(\"**Train Split **\")\n",
    "    model.fit(X_train, y_train)\n",
    "    predict = model.predict(X_test)\n",
    "    R2 = r2_score(y_test, predict)\n",
    "    MSE =  mean_squared_error(y_test,predict)\n",
    "    MAE =  mean_absolute_error(y_test,predict)\n",
    "    RMSE = sqrt(MSE)\n",
    "\n",
    "\n",
    "    print(\"Train City : \", cityTrainName)\n",
    "    print(\"Test City : \", cityTestName)\n",
    "    print(\"MSE : \", MSE)\n",
    "    print(\"MAE : \", MAE)\n",
    "    print(\"R2 : \", R2)\n",
    "    print(\"RMSE : \", RMSE)\n",
    "\n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "    \n",
    "    f_out.write(str(cityTrainName) + \",\")\n",
    "    f_out.write(str(cityTestName) + \",\")\n",
    "    f_out.write(str(MLname) + \",\")\n",
    "    f_out.write('TTS' + \",\")\n",
    "    f_out.write(str(abs(MAE)) + \",\")\n",
    "    f_out.write(str(abs(MSE)) + \",\")\n",
    "    f_out.write(str(RMSE) + \",\")\n",
    "    f_out.write(str(R2) + \",\")\n",
    "    f_out.write(str(len(features_list)) + \",\")\n",
    "    for feature in features_list:\n",
    "        f_out.write(feature + \"&\")\n",
    "    f_out.write(\"\\n\")\n",
    "    \n",
    "\n",
    "    return features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def applyCV(cityTrainName, dataset, cityTestName, MLname, estimator, f_out, Un_needed_columns):\n",
    "    # Construct the pipeline with a standard scaler and a small neural network\n",
    "    estimators = []\n",
    "    estimators.append(('standardize', StandardScaler()))\n",
    "    estimators.append((MLname, estimator))\n",
    "    model = Pipeline(estimators)\n",
    "\n",
    "    # Split the data into features and target label\n",
    "    #Un_needed_columns = ['PM_US Post', 'day']\n",
    "    \n",
    "    # Split the data into features and target label\n",
    "    X = dataset.drop(Un_needed_columns, axis = 1)\n",
    "    y = dataset['PM_US Post']\n",
    "    \n",
    "    # Saving feature names for later use\n",
    "    features_list = list(X.columns)\n",
    "\n",
    "    # We'll use 5-fold cross validation. That is, a random 80% of the data will be used\n",
    "    # to train the model, and the prediction score will be computed on the remaining 20%.\n",
    "    # This process is repeated five times such that the training sets in each \"fold\"\n",
    "    # are mutually orthogonal.\n",
    "    \n",
    "    K = 3\n",
    "    kfold = KFold(n_splits=K,  shuffle=True)\n",
    "\n",
    "    print(estimators)\n",
    "\n",
    "    print(\"**cross_val_score + KFold **\")\n",
    "\n",
    "    results_R2 = cross_val_score(model, X, y, cv=kfold, scoring='r2')\n",
    "    R2 = np.mean(results_R2)\n",
    "    print('CV Scoring Result: r2 : mean=',np.mean(results_R2),'std=',np.std(results_R2))\n",
    "    #print(results_R2) \n",
    "    print(\"**************\")\n",
    "    \n",
    "    results_MAE = cross_val_score(model, X, y, cv=kfold, scoring='neg_mean_absolute_error')\n",
    "    MAE = np.mean(results_MAE)\n",
    "    print('CV Scoring Result: MAE : mean=',np.mean(results_MAE),'std=',np.std(results_MAE))\n",
    "    #print(results_MAE)  \n",
    "    print(\"**************\")\n",
    "\n",
    "    #results_MSE = cross_val_score(model, X, y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "    MSE = 0\n",
    "    #MSE = np.mean(results_MSE)\n",
    "    #print('CV Scoring Result: MSE : mean=',np.mean(results_MSE),'std=',np.std(results_MSE))\n",
    "    #print(results_MSE) \n",
    "    RMSE = 0\n",
    "    #RMSE = sqrt(abs(MSE))\n",
    "\n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "    \n",
    "    f_out.write(str(cityTrainName) + \",\")\n",
    "    f_out.write(str(cityTestName) + \",\")\n",
    "    f_out.write(str(MLname) + \",\")\n",
    "    f_out.write('CV(' + str(K) + \"),\")\n",
    "    f_out.write(str(abs(MAE)) + \",\")\n",
    "    f_out.write(str(abs(MSE)) + \",\")\n",
    "    f_out.write(str(RMSE) + \",\")\n",
    "    f_out.write(str(R2) + \",\")\n",
    "    f_out.write(str(len(features_list)) + \",\")\n",
    "    for feature in features_list:\n",
    "        f_out.write(feature + \"&\")\n",
    "    f_out.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def applyLOO(cityTrainName, dataset, cityTestName, MLname, estimator, f_out, Un_needed_columns):\n",
    "    # Construct the pipeline with a standard scaler and a small neural network\n",
    "    estimators = []\n",
    "    estimators.append(('standardize', StandardScaler()))\n",
    "    estimators.append((MLname, estimator))\n",
    "    model = Pipeline(estimators)\n",
    "\n",
    "    # Split the data into features and target label\n",
    "    #Un_needed_columns = ['PM_US Post', 'day']\n",
    "    \n",
    "    # Split the data into features and target label\n",
    "    X = dataset.drop(Un_needed_columns, axis = 1)\n",
    "    y = dataset['PM_US Post']\n",
    "    \n",
    "    # Saving feature names for later use\n",
    "    features_list = list(X.columns)\n",
    "\n",
    "    # We'll use 5-fold cross validation. That is, a random 80% of the data will be used\n",
    "    # to train the model, and the prediction score will be computed on the remaining 20%.\n",
    "    # This process is repeated five times such that the training sets in each \"fold\"\n",
    "    # are mutually orthogonal.\n",
    "    \n",
    "    groups = dataset['Id']\n",
    "    cv_gen=LeaveOneGroupOut().split(X, y, groups)\n",
    "    cv = list(cv_gen)\n",
    "\n",
    "    print(estimators)\n",
    "\n",
    "    print(\"**cross_val_score + KFold **\")\n",
    "\n",
    "    results_R2 = cross_val_score(model, X, y, cv=cv, scoring='r2')\n",
    "    R2 = np.mean(results_R2)\n",
    "    print('CV Scoring Result: r2 : mean=',np.mean(results_R2),'std=',np.std(results_R2))\n",
    "    #print(results_R2) \n",
    "    print(\"**************\")\n",
    "    \n",
    "    results_MAE = cross_val_score(model, X, y, cv=cv, scoring='neg_mean_absolute_error')\n",
    "    MAE = np.mean(results_MAE)\n",
    "    print('CV Scoring Result: MAE : mean=',np.mean(results_MAE),'std=',np.std(results_MAE))\n",
    "    #print(results_MAE)  \n",
    "    print(\"**************\")\n",
    "\n",
    "    #results_MSE = cross_val_score(model, X, y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "    MSE = 0\n",
    "    #MSE = np.mean(results_MSE)\n",
    "    #print('CV Scoring Result: MSE : mean=',np.mean(results_MSE),'std=',np.std(results_MSE))\n",
    "    #print(results_MSE) \n",
    "    RMSE = 0\n",
    "    #RMSE = sqrt(abs(MSE))\n",
    "\n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "    \n",
    "    f_out.write(str(cityTrainName) + \",\")\n",
    "    f_out.write(str(cityTestName) + \",\")\n",
    "    f_out.write(str(MLname) + \",\")\n",
    "    f_out.write('CV(' + str(K) + \"),\")\n",
    "    f_out.write(str(abs(MAE)) + \",\")\n",
    "    f_out.write(str(abs(MSE)) + \",\")\n",
    "    f_out.write(str(RMSE) + \",\")\n",
    "    f_out.write(str(R2) + \",\")\n",
    "    f_out.write(str(len(features_list)) + \",\")\n",
    "    for feature in features_list:\n",
    "        f_out.write(feature + \"&\")\n",
    "    f_out.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beijing & Chengdu & Shanghai & Shenyang & Guangzhou  ********************** and *********************   Beijing & Chengdu & Shanghai & Shenyang & Guangzhou\n",
      "[('standardize', StandardScaler(copy=True, with_mean=True, with_std=True)), ('LR', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
      "         normalize=False))]\n",
      "**Train Split **\n",
      "Train City :  Beijing & Chengdu & Shanghai & Shenyang & Guangzhou\n",
      "Test City :  Beijing & Chengdu & Shanghai & Shenyang & Guangzhou\n",
      "MSE :  3198.293315615729\n",
      "MAE :  38.62402029630035\n",
      "R2 :  0.24554333590072586\n",
      "RMSE :  56.55345538175125\n",
      "-----------------------------------------------\n",
      "[('standardize', StandardScaler(copy=True, with_mean=True, with_std=True)), ('LR', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
      "         normalize=False))]\n",
      "**cross_val_score + KFold **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:465: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:465: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:465: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:465: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:465: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Scoring Result: r2 : mean= 0.24087153417010254 std= 0.0006964944532678831\n",
      "**************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:465: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:465: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Scoring Result: MAE : mean= -38.84667873169896 std= 0.04939704730660061\n",
      "**************\n",
      "-----------------------------------------------\n",
      "Beijing & Chengdu & Shanghai & Shenyang & Guangzhou  ********************** and *********************   Beijing & Chengdu & Shanghai & Shenyang & Guangzhou\n",
      "[('standardize', StandardScaler(copy=True, with_mean=True, with_std=True)), ('RF', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=96, n_jobs=None,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "**Train Split **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:465: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train City :  Beijing & Chengdu & Shanghai & Shenyang & Guangzhou\n",
      "Test City :  Beijing & Chengdu & Shanghai & Shenyang & Guangzhou\n",
      "MSE :  1297.8185894739167\n",
      "MAE :  22.90766095520151\n",
      "R2 :  0.6938530062768765\n",
      "RMSE :  36.02524933257113\n",
      "-----------------------------------------------\n",
      "[('standardize', StandardScaler(copy=True, with_mean=True, with_std=True)), ('RF', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=96, n_jobs=None,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "**cross_val_score + KFold **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:465: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:465: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:465: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Scoring Result: r2 : mean= 0.6689241091543829 std= 0.00604308416582501\n",
      "**************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:465: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:465: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:465: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Scoring Result: MAE : mean= -23.907093282183116 std= 0.12041959483207636\n",
      "**************\n",
      "-----------------------------------------------\n",
      "Beijing & Chengdu & Shanghai & Shenyang & Guangzhou  ********************** and *********************   Beijing & Chengdu & Shanghai & Shenyang & Guangzhou\n",
      "[('standardize', StandardScaler(copy=True, with_mean=True, with_std=True)), ('ANN', MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(128, 256), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False))]\n",
      "**Train Split **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:465: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train City :  Beijing & Chengdu & Shanghai & Shenyang & Guangzhou\n",
      "Test City :  Beijing & Chengdu & Shanghai & Shenyang & Guangzhou\n",
      "MSE :  1759.0637307894717\n",
      "MAE :  28.76456262373137\n",
      "R2 :  0.5850482669023274\n",
      "RMSE :  41.941193721560566\n",
      "-----------------------------------------------\n",
      "[('standardize', StandardScaler(copy=True, with_mean=True, with_std=True)), ('ANN', MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(128, 256), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False))]\n",
      "**cross_val_score + KFold **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:465: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:465: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:465: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Scoring Result: r2 : mean= 0.563761656626279 std= 0.01581919209968535\n",
      "**************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:465: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:465: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:465: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Scoring Result: MAE : mean= -29.062112494158157 std= 0.19215785217168643\n",
      "**************\n",
      "-----------------------------------------------\n",
      "Beijing & Chengdu & Shanghai & Shenyang & Guangzhou  ********************** and *********************   Beijing & Chengdu & Shanghai & Shenyang & Guangzhou\n",
      "[('standardize', StandardScaler(copy=True, with_mean=True, with_std=True)), ('SVR', SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
      "  gamma='auto_deprecated', kernel='rbf', max_iter=-1, shrinking=True,\n",
      "  tol=0.001, verbose=False))]\n",
      "**Train Split **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:465: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train City :  Beijing & Chengdu & Shanghai & Shenyang & Guangzhou\n",
      "Test City :  Beijing & Chengdu & Shanghai & Shenyang & Guangzhou\n",
      "MSE :  2983.6912015267544\n",
      "MAE :  32.730169385742315\n",
      "R2 :  0.29616658371658466\n",
      "RMSE :  54.623174583017\n",
      "-----------------------------------------------\n",
      "[('standardize', StandardScaler(copy=True, with_mean=True, with_std=True)), ('SVR', SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
      "  gamma='auto_deprecated', kernel='rbf', max_iter=-1, shrinking=True,\n",
      "  tol=0.001, verbose=False))]\n",
      "**cross_val_score + KFold **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:465: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:465: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:465: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Scoring Result: r2 : mean= 0.28325825217188894 std= 0.0010290885154593439\n",
      "**************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:465: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:465: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:465: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Scoring Result: MAE : mean= -33.22634702825169 std= 0.10471634026033942\n",
      "**************\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "with open(\"../China/China_Results/Paper_Results/Level_5_Balanced_Test_22_dayType_features.csv\", 'w') as f_out:\n",
    "    out_colnames = ['Train Site', 'Test Site', 'Algorithm', 'CV', 'MAE', 'MSE', 'RMSE', 'R^2', 'Features_Count', 'Features']        \n",
    "    writer = csv.DictWriter(f_out, fieldnames = out_colnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    Train_city_Name = \"Beijing & Chengdu & Shanghai & Shenyang & Guangzhou\"\n",
    "    Test_city_Name = \"Beijing & Chengdu & Shanghai & Shenyang & Guangzhou\"\n",
    "    \n",
    "    for MLname, ML in MLs.items():\n",
    "        print(Train_city_Name, \" ********************** and *********************  \", Test_city_Name)\n",
    "        Un_needed_columns = ['PM_US Post','day_cos', 'day_sin']\n",
    "        apply_L5(Train_city_Name, train_dataset, Test_city_Name, test_dataset, MLname, ML, f_out, Un_needed_columns)\n",
    "        applyCV(Train_city_Name, dataset, Test_city_Name, MLname, ML, f_out, Un_needed_columns)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "with open(\"../China/China_Results/After_fixing_outliers/Level_5_CV_Unnormalized_Features.csv\", 'w') as f_out:\n",
    "    out_colnames = ['Train Site', 'Test Site', 'Algorithm', 'MAE', 'MSE', 'R^2']        \n",
    "    writer = csv.DictWriter(f_out, fieldnames = out_colnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    Train_city_Name = \"Beijing & Chengdu & Shanghai & Shenyang & Guangzhou\"\n",
    "    Test_city_Name = \"Beijing & Chengdu & Shanghai & Shenyang & Guangzhou\"\n",
    "    \n",
    "    for MLname, ML in MLs.items():\n",
    "        print(Train_city_Name, \" ********************** and *********************  \", Test_city_Name)\n",
    "        applyCV(Train_city_Name, dataset, Test_city_Name, MLname, ML, f_out)\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# Split the data into features and target label\n",
    "Un_needed_columns = ['PM_US Post', 'day', 'DEWP', 'precipitation']\n",
    "# Split the data into features and target label\n",
    "Features = dataset.drop(Un_needed_columns, axis = 1)\n",
    "Target = dataset['PM_US Post']\n",
    "\n",
    "# Saving feature names for later use\n",
    "feature_list = list(Features.columns)\n",
    "\n",
    "display(Features.head(n=2))\n",
    "display(Target.head(n=2))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Features = scaler.fit_transform(Features)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create cross-validation sets from the training data\n",
    "cv_sets = ShuffleSplit(Features.shape[0], n_iter = 3, test_size = 0.30, random_state = 0)\n",
    "    \n",
    "# Create a dictionary for the parameter 'max_depth' with a range from 1 to 40\n",
    "#temp_range = range(1, 600)\n",
    "params = {'n_estimators': [50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600]}\n",
    "\n",
    "scoring_list = ['r2']\n",
    "# Create the grid search cv object --> GridSearchCV()\n",
    "grid = GridSearchCV(estimator = RandomForestRegressor(), param_grid = params, scoring = 'r2', cv = cv_sets)\n",
    "\n",
    "# Fit the grid search object to the data to compute the optimal model\n",
    "grid = grid.fit(Features, Target)\n",
    "\n",
    "best_estimator = grid.best_estimator_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(best_estimator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def apply_Feature_Selection(cityTrainName, Train_dataset, cityTestName, Test_dataset, MLname, estimator, f_out):\n",
    "    # Construct the pipeline with a standard scaler and a small neural network\n",
    "    \n",
    "    #estimators = []\n",
    "    #estimators.append(('standardize', StandardScaler()))\n",
    "    #estimators.append((MLname, estimator))\n",
    "    #model = Pipeline(estimators)\n",
    "    model = estimator\n",
    "    \n",
    "    # Split the data into features and target label\n",
    "    Un_needed_columns = ['PM_US Post', 'day']\n",
    "    \n",
    "    # Split the data into features and target label\n",
    "    X_train = Train_dataset.drop(Un_needed_columns, axis = 1)\n",
    "    X_test = Test_dataset.drop(Un_needed_columns, axis = 1)\n",
    "    \n",
    "    y_train = Train_dataset['PM_US Post']\n",
    "    y_test = Test_dataset['PM_US Post']\n",
    "    \n",
    "    # Saving feature names for later use\n",
    "    feature_list = list(X_train.columns)\n",
    "    \n",
    "    # Feature Scaling\n",
    "    Sc_X = StandardScaler()\n",
    "    X_train = Sc_X.fit_transform(X_train)\n",
    "    X_test = Sc_X.transform(X_test)\n",
    "    \n",
    "    \n",
    "    print(\"**Train Split **\")\n",
    "    model.fit(X_train, y_train)\n",
    "    predict = model.predict(X_test)\n",
    "    R2 = r2_score(y_test, predict)\n",
    "    MSE =  mean_squared_error(y_test,predict)\n",
    "    MAE =  mean_absolute_error(y_test,predict)\n",
    "\n",
    "    print(\"Train City : \", cityTrainName)\n",
    "    print(\"Test City : \", cityTestName)\n",
    "    print(\"MSE : \", MSE)\n",
    "    print(\"MAE : \", MAE)\n",
    "    print(\"R2 : \", R2)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    if(MLname == 'SVR'):\n",
    "        \n",
    "\n",
    "        def f_importances(coef, names):\n",
    "            imp = coef\n",
    "            imp,names = zip(*sorted(zip(imp,names)))\n",
    "            plt.barh(range(len(names)), imp, align='center')\n",
    "            plt.yticks(range(len(names)), names)\n",
    "            plt.show()\n",
    "\n",
    "        f_importances(model.coef_, feature_list)\n",
    "    \n",
    "    \n",
    "    if(MLname == 'RF'):\n",
    "        # Get numerical feature importances\n",
    "        importances = list(model.feature_importances_)\n",
    "\n",
    "        # List of tuples with variable and importance\n",
    "        feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "\n",
    "        # Sort the feature importances by most important first\n",
    "        feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "        # Print out the feature and importances \n",
    "        [print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]\n",
    "    \n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "    f_out.write(str(cityTrainName) + \",\")\n",
    "    f_out.write(str(cityTestName) + \",\")\n",
    "    f_out.write(str(MLname) + \",\")\n",
    "    f_out.write(str(MAE) + \",\")\n",
    "    f_out.write(str(MSE) + \",\")\n",
    "    f_out.write(str(R2) + \",\")\n",
    "    f_out.write(str(('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances) + \"\\n\")\n",
    "\n",
    "    \n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beijing & Chengdu & Shanghai & Shenyang & Guangzhou  ********************** and *********************   Beijing & Chengdu & Shanghai & Shenyang & Guangzhou\n",
      "**Train Split **\n",
      "Train City :  Beijing & Chengdu & Shanghai & Shenyang & Guangzhou\n",
      "Test City :  Beijing & Chengdu & Shanghai & Shenyang & Guangzhou\n",
      "MSE :  1412.9684605444725\n",
      "MAE :  23.49263407247756\n",
      "R2 :  0.6709652137459938\n",
      "Variable: new_wind             Importance: 0.18\n",
      "Variable: TEMP                 Importance: 0.17\n",
      "Variable: HUMI                 Importance: 0.12\n",
      "Variable: PRES                 Importance: 0.12\n",
      "Variable: DEWP                 Importance: 0.1\n",
      "Variable: hour_cos             Importance: 0.05\n",
      "Variable: hour_sin             Importance: 0.05\n",
      "Variable: month_cos            Importance: 0.05\n",
      "Variable: month_sin            Importance: 0.04\n",
      "Variable: year                 Importance: 0.04\n",
      "Variable: cbwd_NE              Importance: 0.01\n",
      "Variable: cbwd_NW              Importance: 0.01\n",
      "Variable: cbwd_SE              Importance: 0.01\n",
      "Variable: cbwd_SW              Importance: 0.01\n",
      "Variable: cbwd_cv              Importance: 0.01\n",
      "Variable: precipitation        Importance: 0.01\n",
      "Variable: season_4             Importance: 0.01\n",
      "Variable: season_1             Importance: 0.0\n",
      "Variable: season_2             Importance: 0.0\n",
      "Variable: season_3             Importance: 0.0\n",
      "-----------------------------------------------\n",
      "Beijing & Chengdu & Shanghai & Shenyang & Guangzhou  ********************** and *********************   Beijing & Chengdu & Shanghai & Shenyang & Guangzhou\n",
      "**Train Split **\n",
      "Train City :  Beijing & Chengdu & Shanghai & Shenyang & Guangzhou\n",
      "Test City :  Beijing & Chengdu & Shanghai & Shenyang & Guangzhou\n",
      "MSE :  3051.089724537966\n",
      "MAE :  32.84908324372305\n",
      "R2 :  0.2894996007424707\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "coef_ is only available when using a linear kernel",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-bcf28128277a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mMLname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mML\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mMLs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain_city_Name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" ********************** and *********************  \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTest_city_Name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mfeatures_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_Feature_Selection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain_city_Name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTest_city_Name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMLname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mML\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-f31663a01b9a>\u001b[0m in \u001b[0;36mapply_Feature_Selection\u001b[0;34m(cityTrainName, Train_dataset, cityTestName, Test_dataset, MLname, estimator, f_out)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mf_importances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mcoef_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcoef_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'linear'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m             raise AttributeError('coef_ is only available when using a '\n\u001b[0m\u001b[1;32m    466\u001b[0m                                  'linear kernel')\n\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: coef_ is only available when using a linear kernel"
     ]
    }
   ],
   "source": [
    "with open(\"../China/China_Results/After_fixing_outliers/Feature_Selection/Level_5_Balanced_Test_FS_Wrapper.csv\", 'w') as f_out:\n",
    "    out_colnames = ['Train Site', 'Test Site', 'Algorithm', 'MAE', 'MSE', 'R^2', 'Features & importance']        \n",
    "    writer = csv.DictWriter(f_out, fieldnames = out_colnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    Train_city_Name = \"Beijing & Chengdu & Shanghai & Shenyang & Guangzhou\"\n",
    "    Test_city_Name = \"Beijing & Chengdu & Shanghai & Shenyang & Guangzhou\"\n",
    "    \n",
    "    MLs = {'RF': RF, 'SVR' : SVR}\n",
    "\n",
    "    for MLname, ML in MLs.items():\n",
    "        print(Train_city_Name, \" ********************** and *********************  \", Test_city_Name)\n",
    "        features_list = apply_Feature_Selection(Train_city_Name, train_dataset, Test_city_Name, test_dataset, MLname, ML, f_out)\n",
    "    \n",
    "    for feature in features_list:\n",
    "        f_out.write(feature + \"&\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEWP</th>\n",
       "      <th>HUMI</th>\n",
       "      <th>PRES</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>cbwd_NE</th>\n",
       "      <th>cbwd_NW</th>\n",
       "      <th>cbwd_SE</th>\n",
       "      <th>cbwd_SW</th>\n",
       "      <th>cbwd_cv</th>\n",
       "      <th>day</th>\n",
       "      <th>...</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>new_wind</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>season_1</th>\n",
       "      <th>season_2</th>\n",
       "      <th>season_3</th>\n",
       "      <th>season_4</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-10.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>1018.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-11.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1017.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   DEWP  HUMI    PRES  TEMP  cbwd_NE  cbwd_NW  cbwd_SE  cbwd_SW  cbwd_cv  day  \\\n",
       "0 -10.0  67.0  1018.0  -5.0        0        1        0        0        0    1   \n",
       "1 -11.0  73.0  1017.0  -7.0        0        1        0        0        0    1   \n",
       "\n",
       "   ...   hour_sin  month_cos  month_sin  new_wind  precipitation  season_1  \\\n",
       "0  ...   0.000000        1.0        0.0      4.02            0.0         0   \n",
       "1  ...   0.258819        1.0        0.0      4.02            0.0         0   \n",
       "\n",
       "   season_2  season_3  season_4  year  \n",
       "0         0         0         1  2013  \n",
       "1         0         0         1  2013  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    31.0\n",
       "1    32.0\n",
       "Name: PM_US Post, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso model:  14.806 * month_cos + -12.392 * cbwd_NE + 11.012 * cbwd_cv + -7.454 * year + -7.159 * new_wind + 6.708 * cbwd_SE + -6.366 * hour_cos + -6.265 * hour_sin + -5.692 * cbwd_NW + 2.402 * season_4 + -2.334 * TEMP + -2.143 * precipitation + 1.294 * month_sin + -1.076 * season_3 + 0.932 * DEWP + 0.375 * day + 0.069 * HUMI + -0.0 * PRES + -0.0 * cbwd_SW + 0.0 * season_1 + -0.0 * season_2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "  \n",
    "# Split the data into features and target label\n",
    "Features = dataset.drop('PM_US Post', axis = 1)\n",
    "Target = dataset['PM_US Post']\n",
    "\n",
    "feature_name = Features.columns.tolist()\n",
    "\n",
    "display(Features.head(n=2))\n",
    "display(Target.head(n=2))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "ScaledFeatures = scaler.fit_transform(Features)\n",
    "  \n",
    "lasso = Lasso(alpha=.3)\n",
    "lasso.fit(Features, Target)\n",
    "  \n",
    "print (\"Lasso model: \", pretty_print_linear(lasso.coef_, feature_name, sort = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A helper method for pretty-printing linear models\n",
    "def pretty_print_linear(coefs, names = None, sort = False):\n",
    "    if names == None:\n",
    "        names = [\"X%s\" % x for x in range(len(coefs))]\n",
    "    lst = zip(coefs, names)\n",
    "    if sort:\n",
    "        lst = sorted(lst,  key = lambda x:-np.abs(x[0]))\n",
    "    return \" + \".join(\"%s * %s\" % (round(coef, 3), name)\n",
    "                                   for coef, name in lst)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
